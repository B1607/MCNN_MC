{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271c45cf-e40c-417c-a83e-03b375add143",
   "metadata": {},
   "source": [
    "# Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd76d2-6c8b-4901-896e-85c3a298082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from time import gmtime, strftime\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,Model\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "import argparse\n",
    "\n",
    "import loading_data as load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e9da3-3fb9-4ed6-a197-380544b91cf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setting of parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851d5dd-21d4-4182-8fa5-2027de0091de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXSEQ = 800\n",
    "#The setting of sequence length.\n",
    "\n",
    "DATA_TYPE = \"prottrans\"\n",
    "#The type of data. Options are \"ProtTrans\", \"tape\", \"esm2\".\n",
    "\n",
    "NUM_FEATURE = 1024\n",
    "#\"The number of data feature dimensions. 1024 for ProtTrans, 768 for tape, 1280 for esm2.\"\n",
    "\n",
    "NUM_FILTER = 128\n",
    "#The number of filters in the convolutional layer.\n",
    "\n",
    "NUM_HIDDEN = 1000#100\n",
    "#The number of hidden units in the dense layer.\n",
    "\n",
    "BATCH_SIZE  = 1024\n",
    "#The batch size for training the model.\n",
    "\n",
    "WINDOW_SIZES = [4,8,16]\n",
    "#The window sizes for convolutional filters.\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "CLASS_NAMES = ['Negative','Positive']\n",
    "#The label of dataset.\n",
    "\n",
    "EPOCHS      = 1\n",
    "#The number of epochs for training the model.\n",
    "\n",
    "K_Fold = 5\n",
    "#The number of n-fold cross validation.\n",
    "\n",
    "VALIDATION_MODE=\"independent\"\n",
    "#The validation mode. Options are \"cross\", \"independent\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb44729-9da2-4f49-b84a-3f29e1fae1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMCNN_MC\\n\")\n",
    "print(\"The setting of sequence length: \",MAXSEQ)\n",
    "print(\"The number of filters in the convolutional layer: \",NUM_FILTER)\n",
    "print(\"The number of hidden units in the dense layer: \",NUM_HIDDEN)\n",
    "print(\"The batch size for training the model: \",BATCH_SIZE)\n",
    "print(\"The window sizes for convolutional filters: \",WINDOW_SIZES)\n",
    "print(\"The validation mode: \",VALIDATION_MODE)\n",
    "print(\"The type of data: \",DATA_TYPE)\n",
    "print(\"The number of data feature dimensions: \",NUM_FEATURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf4caa-b045-4a63-8dab-cf66dd529db1",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abe117-0b24-4a98-8f1e-57ad06c9e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fit batch funtion\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, labels, batch_size):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_data = [self.data[i] for i in batch_indexes]\n",
    "        batch_labels = [self.labels[i] for i in batch_indexes]\n",
    "        return np.array(batch_data), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad8130-4a7e-45ae-b873-d21cf32fd367",
   "metadata": {},
   "source": [
    "# MCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c0e76-2144-4ea4-a3d3-c66e0c563930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepScan(Model):\n",
    "    def __init__(self,\n",
    "                 input_shape=(1, MAXSEQ, NUM_FEATURE),\n",
    "                 window_sizes=[32],\n",
    "                 num_filters=256,\n",
    "                 num_hidden=1000):\n",
    "        # Initialize the parent class\n",
    "        super(DeepScan, self).__init__()\n",
    "        \n",
    "        # Initialize the input layer\n",
    "        self.input_layer = tf.keras.Input(input_shape)\n",
    "        \n",
    "        # Initialize convolution window sizes\n",
    "        self.window_sizes = window_sizes\n",
    "        \n",
    "        # Initialize lists to store convolution, pooling, and flatten layers\n",
    "        self.conv2d = []\n",
    "        self.maxpool = []\n",
    "        self.flatten = []\n",
    "        \n",
    "        # Create corresponding convolution, pooling, and flatten layers for each window size\n",
    "        for window_size in self.window_sizes:\n",
    "            self.conv2d.append(\n",
    "                layers.Conv2D(filters=num_filters,\n",
    "                              kernel_size=(1, window_size),\n",
    "                              activation=tf.nn.relu,\n",
    "                              padding='valid',\n",
    "                              bias_initializer=tf.constant_initializer(0.1),\n",
    "                              kernel_initializer=tf.keras.initializers.GlorotUniform())\n",
    "            )\n",
    "            self.maxpool.append(\n",
    "                layers.MaxPooling2D(pool_size=(1, MAXSEQ - window_size + 1),\n",
    "                                    strides=(1, MAXSEQ),\n",
    "                                    padding='valid')\n",
    "            )\n",
    "            self.flatten.append(\n",
    "                layers.Flatten()\n",
    "            )\n",
    "        \n",
    "        # Initialize Dropout layer to prevent overfitting\n",
    "        self.dropout = layers.Dropout(rate=0.7)\n",
    "        \n",
    "        # Initialize the first fully connected layer\n",
    "        self.fc1 = layers.Dense(num_hidden,\n",
    "                                activation=tf.nn.relu,\n",
    "                                bias_initializer=tf.constant_initializer(0.1),\n",
    "                                kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "        )\n",
    "        \n",
    "        # Initialize the output layer with softmax activation\n",
    "        self.fc2 = layers.Dense(NUM_CLASSES,\n",
    "                                activation='softmax',\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(1e-3)\n",
    "        )\n",
    "        \n",
    "        # Get the output layer by calling the call method\n",
    "        self.out = self.call(self.input_layer)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # List to store outputs of convolution, pooling, and flatten layers\n",
    "        _x = []\n",
    "        \n",
    "        # Perform convolution, pooling, and flatten operations for each window size\n",
    "        for i in range(len(self.window_sizes)):\n",
    "            x_conv = self.conv2d[i](x)\n",
    "            x_maxp = self.maxpool[i](x_conv)\n",
    "            x_flat = self.flatten[i](x_maxp)\n",
    "            _x.append(x_flat)\n",
    "        \n",
    "        # Concatenate the outputs of all flatten layers\n",
    "        x = tf.concat(_x, 1)\n",
    "        \n",
    "        # Apply Dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Apply the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Apply the output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd6e30-ad98-4e37-a76e-92cd7a39cfc6",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767da2b2-407b-45ab-8840-8056249aa086",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test= load_data.MCNN_data_load() #Load dataset from import_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f495f-a64c-40e5-bbf2-fd8874023924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of training dataset :\",x_train.shape)\n",
    "print(\"The data type of training dataset :\",x_train.dtype)\n",
    "print(\"The shape of training label :\",y_train.shape)\n",
    "print(\"The shape of validation dataset :\",x_test.shape)\n",
    "print(\"The data type of validation dataset :\",x_test.dtype)\n",
    "print(\"The shape of validation label :\",y_test.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f0a3d-be25-4106-a4b1-a4bee9f1aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model, x_test, y_test):\n",
    "    \n",
    "    # Generate predictions for the test data\n",
    "    pred_test = model.predict(x_test)\n",
    "    \n",
    "    # Calculate the false positive rate, true positive rate, and thresholds\n",
    "    fpr, tpr, thresholds = roc_curve(y_test[:, 1], pred_test[:, 1])\n",
    "    # Calculate the Area Under the Curve (AUC) for the ROC curve\n",
    "    AUC = metrics.auc(fpr, tpr)\n",
    "    # Display the ROC curve\n",
    "    if (VALIDATION_MODE!=\"cross\"):\n",
    "        display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=AUC, estimator_name='mCNN')\n",
    "        display.plot()\n",
    "    \n",
    "    # Calculate the geometric mean for each threshold\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    # Locate the index of the largest geometric mean\n",
    "    ix = np.argmax(gmeans)\n",
    "    print(f'\\nBest Threshold={thresholds[ix]}, G-Mean={gmeans[ix]}')\n",
    "    # Set the threshold to the one with the highest geometric mean\n",
    "    threshold = thresholds[ix]\n",
    "    # Generate binary predictions based on the threshold\n",
    "    y_pred = (pred_test[:, 1] >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix values: TN, FP, FN, TP\n",
    "    TN, FP, FN, TP = metrics.confusion_matrix(y_test[:, 1], y_pred).ravel()\n",
    "    # Calculate Sensitivity (Recall)\n",
    "    Sens = TP / (TP + FN) if TP + FN > 0 else 0.0\n",
    "    # Calculate Specificity\n",
    "    Spec = TN / (FP + TN) if FP + TN > 0 else 0.0\n",
    "    # Calculate Accuracy\n",
    "    Acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    # Calculate Matthews Correlation Coefficient (MCC)\n",
    "    MCC = (TP * TN - FP * FN) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) if TP + FP > 0 and FP + TN > 0 and TP + FN and TN + FN else 0.0\n",
    "    # Calculate F1 Score\n",
    "    F1 = 2 * TP / (2 * TP + FP + FN)\n",
    "    # Calculate Precision\n",
    "    Prec = TP / (TP + FP)\n",
    "    # Calculate Recall\n",
    "    Recall = TP / (TP + FN)\n",
    "    \n",
    "    # Print the performance metrics\n",
    "    print(f'TP={TP}, FP={FP}, TN={TN}, FN={FN}, Sens={Sens:.4f}, Spec={Spec:.4f}, Acc={Acc:.4f}, MCC={MCC:.4f}, AUC={AUC:.4f}, F1={F1:.4f}, Prec={Prec:.4f}, Recall={Recall:.4f}\\n')\n",
    "    \n",
    "    # Return the performance metrics\n",
    "    return TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC, F1, Prec, Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314bd9a-10f4-40ef-a0b1-2d3788a6ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(VALIDATION_MODE == \"cross\"):\n",
    "    # Initialize K-Fold cross-validation\n",
    "    kfold = KFold(n_splits=K_Fold, shuffle=True, random_state=2)\n",
    "    \n",
    "    results = []  # List to store results of each fold\n",
    "    i = 1  # Counter for fold number\n",
    "    \n",
    "    # Iterate over each split of the dataset\n",
    "    for train_index, test_index in kfold.split(x_train):\n",
    "        print(f\"{i} / {K_Fold}\\n\")\n",
    "        \n",
    "        # Split the data into training and testing sets for the current fold\n",
    "        X_train, X_test = x_train[train_index], x_train[test_index]\n",
    "        Y_train, Y_test = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "        # Print the shapes of the training and testing datasets\n",
    "        print(\"The shape of training dataset of cross validation:\", X_train.shape)\n",
    "        print(\"The shape of training label of cross validation:\", Y_train.shape)\n",
    "        print(\"The shape of validation dataset of cross validation:\", X_test.shape)\n",
    "        print(\"The shape of validation label of cross validation:\", Y_test.shape)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Create a data generator for the training data\n",
    "        generator = DataGenerator(X_train, Y_train, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        # Initialize the DeepScan model\n",
    "        model = DeepScan(\n",
    "            num_filters=NUM_FILTER,\n",
    "            num_hidden=NUM_HIDDEN,\n",
    "            window_sizes=WINDOW_SIZES\n",
    "        )\n",
    "        \n",
    "        # Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Build the model with the input shape of the training data\n",
    "        model.build(input_shape=X_train.shape)\n",
    "        \n",
    "        # Print the model summary\n",
    "        model.summary()\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            generator,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)],\n",
    "            verbose=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Test the model on the validation set and get performance metrics\n",
    "        TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC, F1, Prec, Recall = model_test(model, X_test, Y_test)\n",
    "        \n",
    "        # Append the results to the list\n",
    "        results.append([TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC, F1, Prec, Recall])\n",
    "        \n",
    "        # Increment the fold counter\n",
    "        i += 1\n",
    "        \n",
    "        # Clear the training and testing data from memory\n",
    "        del X_train\n",
    "        del X_test\n",
    "        del Y_train\n",
    "        del Y_test\n",
    "        gc.collect()\n",
    "    \n",
    "    # Calculate the mean results across all folds\n",
    "    mean_results = np.mean(results, axis=0)\n",
    "    \n",
    "    # Print the mean results of the cross-validation\n",
    "    print(f\"The mean of {K_Fold}-Fold cross-validation results:\")\n",
    "    print(f'TP={mean_results[0]:.4}, FP={mean_results[1]:.4}, TN={mean_results[2]:.4}, FN={mean_results[3]:.4}, '\n",
    "          f'Sens={mean_results[4]:.4}, Spec={mean_results[5]:.4}, Acc={mean_results[6]:.4}, MCC={mean_results[7]:.4}, AUC={mean_results[8]:.4}, F1={mean_results[9]:.4}, Prec={mean_results[10]:.4}, Recall={mean_results[10]:.4}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa9cbe-3dc7-41a0-af37-8b2baffd372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(VALIDATION_MODE == \"independent\"):\n",
    "    # Create a data generator for the training data\n",
    "    generator = DataGenerator(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize the DeepScan model\n",
    "    model = DeepScan(\n",
    "        num_filters=NUM_FILTER,\n",
    "        num_hidden=NUM_HIDDEN,\n",
    "        window_sizes=WINDOW_SIZES\n",
    "    )\n",
    "    \n",
    "    # Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Build the model with the input shape of the training data\n",
    "    model.build(input_shape=x_train.shape)\n",
    "    \n",
    "    # Print the model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        generator,\n",
    "        epochs=EPOCHS,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    # Test the model on the independent test set and get performance metrics\n",
    "    TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC, F1, Prec, Recall = model_test(model, x_test, y_test)\n",
    "    \n",
    "    # Print the performance metrics\n",
    "    print(f'TP={TP}, FP={FP}, TN={TN}, FN={FN}, Sens={Sens:.4f}, Spec={Spec:.4f}, Acc={Acc:.4f}, MCC={MCC:.4f}, AUC={AUC:.4f}, F1={F1:.4f}, Prec={Prec:.4f}, Recall={Recall:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bfa560-7be0-4d44-a4e4-0e01f591be4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
